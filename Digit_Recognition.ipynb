{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(data_file, label_file):\n",
    "    # Create dataframe\n",
    "    data = []\n",
    "    label = []\n",
    "\n",
    "    # For each line in the two files\n",
    "    for line_data, line_label in zip_longest(open(data_file), open(label_file)):\n",
    "        # Add data to our array\n",
    "        data.append(list(map(float, line_data.split(','))))\n",
    "        \n",
    "        # Add label to our array\n",
    "        label.append(int(line_label))\n",
    "    \n",
    "    return np.array(data), np.array(label)\n",
    "\n",
    "X, y = extract_data(\"image_0.txt\", \"label.txt\")\n",
    "y[y == 10.] = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Separate data between training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, percentage=0.8):\n",
    "    nb_train_lines = round(500 * percentage)\n",
    "    nb_test_lines = 500 - nb_train_lines\n",
    "    \n",
    "    X_train = np.empty((10 * nb_train_lines, 400))\n",
    "    y_train = np.empty((10 * nb_train_lines))\n",
    "    X_test = np.empty((10 * nb_test_lines, 400))\n",
    "    y_test = np.empty((10 * nb_test_lines))\n",
    "    \n",
    "    for i in range(0, 10):        \n",
    "        X_train[i * nb_train_lines:(i + 1) * nb_train_lines] = X[i * 500:(i * 500) + nb_train_lines]\n",
    "        y_train[i * nb_train_lines:(i + 1) * nb_train_lines] = y[i * 500:(i * 500) + nb_train_lines]\n",
    "        X_test[i * nb_test_lines:(i + 1) * nb_test_lines] = X[(i * 500) + nb_train_lines:(i + 1) * 500]\n",
    "        y_test[i * nb_test_lines:(i + 1) * nb_test_lines] = y[(i * 500) + nb_train_lines:(i + 1) * 500]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Implement the feed-forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class FFNN:\n",
    "    \"\"\" Feed-Forward Neural Network with one hidden layer. \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_layer_size):\n",
    "        self.nb_layers = 3\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Input with intercept\n",
    "        self.X_intercept = np.empty(0)\n",
    "        \n",
    "        # First level of weights\n",
    "        self.w1 = np.random.randn(input_size + 1, self.hidden_layer_size)\n",
    "        \n",
    "        # First layer output\n",
    "        self.fst_out = np.empty(0)\n",
    "    \n",
    "        # Second level of weights\n",
    "        self.w2 = np.random.randn(self.hidden_layer_size + 1, 10)\n",
    "        \n",
    "        # Second layer output\n",
    "        self.snd_out = np.empty(0)\n",
    "        \n",
    "        # Real output\n",
    "        self.Y = np.empty(0)\n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "        self.Y = Y\n",
    "        \n",
    "        # Add intercept column (column of ones)\n",
    "        self.X_intercept = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "        \n",
    "        # Multiply by the weights\n",
    "        self.fst_out = np.dot(self.X_intercept, self.w1)\n",
    "        \n",
    "        # Go through the first activation (sigmoid)\n",
    "        self.fst_out = sigmoid(self.fst_out)\n",
    "        \n",
    "        # Add intercept column (column of ones)\n",
    "        self.fst_out = np.concatenate((np.ones((self.fst_out.shape[0], 1)), self.fst_out), axis=1)\n",
    "        \n",
    "        # Multiply by the weights\n",
    "        self.snd_out = np.dot(self.fst_out, self.w2)\n",
    "        \n",
    "        # Go through the second activation (sigmoid)\n",
    "        self.snd_out = sigmoid(self.snd_out)\n",
    "    \n",
    "    def accuracy(self):\n",
    "        nb_right = 0\n",
    "        \n",
    "        for pred, y in zip(self.snd_out, self.Y):\n",
    "            if y[np.argmax(pred)] == 1:\n",
    "                nb_right += 1\n",
    "        \n",
    "        return 100 * nb_right / self.Y.shape[0]\n",
    "    \n",
    "    def backward(self):\n",
    "        learning_rate = 0.1\n",
    "        \n",
    "        # For all the weights of the first layer\n",
    "        for n in range(self.input_size + 1):\n",
    "            for k in range(1, self.hidden_layer_size):\n",
    "                # First layer weights update\n",
    "                derror_w1 = (self.snd_out - self.Y) * self.snd_out * (1 - self.snd_out)\n",
    "                \n",
    "                derror_w1 *= self.w2[k]\n",
    "                \n",
    "                f = np.delete(self.fst_out, 0, axis=1) # we delete the intercept column\n",
    "                derror_w1 *= (f[:, k] * (1 - f[:, k]) * self.X_intercept[:, n])[:, np.newaxis]\n",
    "                \n",
    "                self.w1[n][k] -= learning_rate * derror_w1.sum()\n",
    "                \n",
    "        # For all the weights of the first layer\n",
    "        for k in range(self.hidden_layer_size):\n",
    "            for j in range(1, self.snd_out.shape[1]):\n",
    "                # Second layer weights update\n",
    "                derror_w2 = (self.snd_out - self.Y) * self.snd_out * (1 - self.snd_out)\n",
    "                \n",
    "                derror_w2 *= self.fst_out[:, k][:, np.newaxis]\n",
    "                \n",
    "                self.w2[k][j] -= learning_rate * derror_w2.sum()\n",
    "        \n",
    "        \n",
    "\n",
    "ffnn = FFNN(X_train.shape[1], 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.zeros((len(y_train), 10))\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    Y_train[i][int(y_train[i])] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 - Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X, Y):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(X)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(Y)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.6 %\n",
      "1 9.7 %\n",
      "2 9.4 %\n",
      "3 7.7 %\n",
      "4 10.6 %\n",
      "5 10.7 %\n",
      "6 10.1 %\n",
      "7 10.9 %\n",
      "8 10.7 %\n",
      "9 10.2 %\n",
      "10 10.3 %\n",
      "11 10.5 %\n",
      "12 10.3 %\n",
      "13 8.9 %\n",
      "14 10.6 %\n",
      "15 10.6 %\n",
      "16 8.2 %\n",
      "17 10.2 %\n",
      "18 10.9 %\n",
      "19 9.5 %\n",
      "20 10.6 %\n",
      "21 10.5 %\n",
      "22 9.7 %\n",
      "23 9.6 %\n",
      "24 9.7 %\n",
      "25 9.6 %\n",
      "26 9.7 %\n",
      "27 9.7 %\n",
      "28 9.0 %\n",
      "29 10.6 %\n",
      "30 9.7 %\n",
      "31 10.2 %\n",
      "32 8.9 %\n",
      "33 10.8 %\n",
      "34 10.0 %\n",
      "35 9.7 %\n",
      "36 11.8 %\n",
      "37 9.9 %\n",
      "38 10.0 %\n",
      "39 9.6 %\n",
      "40 9.0 %\n",
      "41 8.5 %\n",
      "42 10.2 %\n",
      "43 11.4 %\n",
      "44 9.4 %\n",
      "45 11.0 %\n",
      "46 11.8 %\n",
      "47 11.0 %\n",
      "48 9.8 %\n",
      "49 9.4 %\n",
      "50 9.8 %\n",
      "51 10.9 %\n",
      "52 10.2 %\n",
      "53 9.6 %\n",
      "54 11.0 %\n",
      "55 9.6 %\n",
      "56 10.7 %\n",
      "57 9.9 %\n",
      "58 8.4 %\n",
      "59 10.0 %\n",
      "60 8.9 %\n",
      "61 10.5 %\n",
      "62 11.6 %\n",
      "63 10.0 %\n",
      "64 10.9 %\n",
      "65 10.8 %\n",
      "66 10.0 %\n",
      "67 10.1 %\n",
      "68 9.4 %\n",
      "69 9.8 %\n",
      "70 10.9 %\n",
      "71 9.3 %\n",
      "72 7.9 %\n",
      "73 8.8 %\n",
      "74 10.0 %\n",
      "75 9.7 %\n",
      "76 9.5 %\n",
      "77 9.6 %\n",
      "78 9.8 %\n",
      "79 11.1 %\n",
      "80 12.6 %\n",
      "81 10.3 %\n",
      "82 10.6 %\n",
      "83 10.0 %\n",
      "84 11.5 %\n",
      "85 9.9 %\n",
      "86 9.3 %\n",
      "87 8.7 %\n",
      "88 9.6 %\n",
      "89 11.0 %\n",
      "90 9.4 %\n",
      "91 10.6 %\n",
      "92 9.5 %\n",
      "93 9.4 %\n",
      "94 9.6 %\n",
      "95 9.3 %\n",
      "96 10.7 %\n",
      "97 10.1 %\n",
      "98 9.0 %\n",
      "99 10.3 %\n",
      "100 10.2 %\n",
      "101 9.1 %\n",
      "102 9.7 %\n",
      "103 10.7 %\n",
      "104 9.0 %\n",
      "105 11.3 %\n",
      "106 10.6 %\n",
      "107 10.6 %\n",
      "108 10.1 %\n",
      "109 10.2 %\n",
      "110 9.5 %\n",
      "111 9.0 %\n",
      "112 10.8 %\n",
      "113 10.5 %\n",
      "114 8.9 %\n",
      "115 10.0 %\n",
      "116 10.5 %\n",
      "117 8.8 %\n",
      "118 11.3 %\n",
      "119 10.2 %\n",
      "120 12.2 %\n",
      "121 9.4 %\n",
      "122 10.8 %\n",
      "123 9.8 %\n",
      "124 9.1 %\n",
      "125 10.6 %\n",
      "126 9.9 %\n",
      "127 9.5 %\n",
      "128 10.1 %\n",
      "129 10.0 %\n",
      "130 9.8 %\n",
      "131 9.7 %\n",
      "132 9.7 %\n",
      "133 9.6 %\n",
      "134 10.9 %\n",
      "135 9.8 %\n",
      "136 10.2 %\n",
      "137 10.1 %\n",
      "138 9.0 %\n",
      "139 9.5 %\n",
      "140 10.3 %\n",
      "141 8.9 %\n",
      "142 11.0 %\n",
      "143 11.1 %\n",
      "144 10.9 %\n",
      "145 9.1 %\n",
      "146 10.8 %\n",
      "147 9.4 %\n",
      "148 8.1 %\n",
      "149 9.5 %\n",
      "150 10.3 %\n",
      "151 10.7 %\n",
      "152 10.3 %\n",
      "153 10.1 %\n",
      "154 10.8 %\n",
      "155 9.9 %\n",
      "156 9.8 %\n",
      "157 9.6 %\n",
      "158 10.5 %\n",
      "159 10.4 %\n",
      "160 10.5 %\n",
      "161 10.2 %\n",
      "162 10.4 %\n",
      "163 9.1 %\n",
      "164 12.2 %\n",
      "165 9.5 %\n",
      "166 10.2 %\n",
      "167 11.6 %\n",
      "168 9.0 %\n",
      "169 10.3 %\n",
      "170 10.6 %\n",
      "171 9.9 %\n",
      "172 9.5 %\n",
      "173 10.0 %\n",
      "174 10.3 %\n",
      "175 10.0 %\n",
      "176 10.8 %\n",
      "177 10.2 %\n",
      "178 10.3 %\n",
      "179 11.1 %\n",
      "180 10.6 %\n",
      "181 9.3 %\n",
      "182 10.2 %\n",
      "183 8.4 %\n",
      "184 10.2 %\n",
      "185 10.0 %\n",
      "186 9.5 %\n",
      "187 9.6 %\n",
      "188 9.5 %\n",
      "189 10.0 %\n",
      "190 8.9 %\n",
      "191 10.2 %\n",
      "192 9.2 %\n",
      "193 11.3 %\n",
      "194 9.9 %\n",
      "195 8.7 %\n",
      "196 8.7 %\n",
      "197 10.0 %\n",
      "198 11.3 %\n",
      "199 10.5 %\n",
      "200 10.4 %\n",
      "201 10.4 %\n",
      "202 8.7 %\n",
      "203 8.5 %\n",
      "204 9.7 %\n",
      "205 9.7 %\n",
      "206 10.4 %\n",
      "207 9.6 %\n",
      "208 9.5 %\n",
      "209 10.1 %\n",
      "210 10.7 %\n",
      "211 10.5 %\n",
      "212 10.1 %\n",
      "213 9.2 %\n",
      "214 9.0 %\n",
      "215 9.0 %\n",
      "216 10.3 %\n",
      "217 9.6 %\n",
      "218 10.1 %\n",
      "219 10.3 %\n",
      "220 10.7 %\n",
      "221 9.0 %\n",
      "222 8.4 %\n",
      "223 9.5 %\n",
      "224 10.6 %\n",
      "225 10.2 %\n",
      "226 10.0 %\n",
      "227 9.6 %\n",
      "228 10.1 %\n",
      "229 10.0 %\n",
      "230 11.8 %\n",
      "231 10.1 %\n",
      "232 8.9 %\n",
      "233 10.4 %\n",
      "234 10.4 %\n",
      "235 9.1 %\n",
      "236 9.9 %\n",
      "237 10.2 %\n",
      "238 10.8 %\n",
      "239 9.1 %\n",
      "240 9.1 %\n",
      "241 8.7 %\n",
      "242 9.3 %\n",
      "243 10.5 %\n",
      "244 10.5 %\n",
      "245 11.3 %\n",
      "246 11.0 %\n",
      "247 10.4 %\n",
      "248 9.6 %\n",
      "249 9.5 %\n",
      "250 9.6 %\n",
      "251 11.3 %\n",
      "252 9.5 %\n",
      "253 9.3 %\n",
      "254 10.7 %\n",
      "255 8.8 %\n",
      "256 10.6 %\n",
      "257 9.4 %\n",
      "258 10.1 %\n",
      "259 10.0 %\n",
      "260 9.7 %\n",
      "261 9.6 %\n",
      "262 9.6 %\n",
      "263 9.5 %\n",
      "264 10.7 %\n",
      "265 8.9 %\n",
      "266 10.2 %\n",
      "267 11.4 %\n",
      "268 9.5 %\n",
      "269 10.5 %\n",
      "270 10.2 %\n",
      "271 10.2 %\n",
      "272 9.2 %\n",
      "273 9.7 %\n",
      "274 10.1 %\n",
      "275 9.4 %\n",
      "276 9.9 %\n",
      "277 10.7 %\n",
      "278 11.1 %\n",
      "279 9.9 %\n",
      "280 10.2 %\n",
      "281 10.9 %\n",
      "282 9.9 %\n",
      "283 10.5 %\n",
      "284 10.5 %\n",
      "285 10.1 %\n",
      "286 11.0 %\n",
      "287 9.4 %\n",
      "288 10.5 %\n",
      "289 10.5 %\n",
      "290 11.0 %\n",
      "291 10.4 %\n",
      "292 9.7 %\n",
      "293 9.3 %\n",
      "294 10.0 %\n",
      "295 10.0 %\n",
      "296 9.8 %\n",
      "297 10.2 %\n",
      "298 10.9 %\n",
      "299 10.1 %\n",
      "300 9.7 %\n",
      "301 9.4 %\n",
      "302 9.3 %\n",
      "303 9.7 %\n",
      "304 10.0 %\n",
      "305 10.0 %\n",
      "306 10.8 %\n",
      "307 10.6 %\n",
      "308 9.4 %\n",
      "309 10.3 %\n",
      "310 9.0 %\n",
      "311 10.6 %\n",
      "312 9.3 %\n",
      "313 10.1 %\n",
      "314 10.8 %\n",
      "315 9.1 %\n",
      "316 9.8 %\n",
      "317 9.9 %\n",
      "318 10.0 %\n",
      "319 9.7 %\n",
      "320 9.9 %\n",
      "321 10.4 %\n",
      "322 8.6 %\n",
      "323 8.9 %\n",
      "324 9.4 %\n",
      "325 9.1 %\n",
      "326 10.8 %\n",
      "327 10.3 %\n",
      "328 9.4 %\n",
      "329 10.0 %\n",
      "330 9.6 %\n",
      "331 9.1 %\n",
      "332 10.1 %\n",
      "333 9.0 %\n",
      "334 10.6 %\n",
      "335 9.7 %\n",
      "336 11.2 %\n",
      "337 12.5 %\n",
      "338 11.5 %\n",
      "339 11.0 %\n",
      "340 9.6 %\n",
      "341 11.0 %\n",
      "342 9.5 %\n",
      "343 10.6 %\n",
      "344 10.1 %\n",
      "345 8.8 %\n",
      "346 10.8 %\n",
      "347 9.3 %\n",
      "348 10.0 %\n",
      "349 9.1 %\n",
      "350 10.6 %\n",
      "351 9.3 %\n",
      "352 9.6 %\n",
      "353 10.9 %\n",
      "354 9.5 %\n",
      "355 9.9 %\n",
      "356 9.2 %\n",
      "357 9.3 %\n",
      "358 11.1 %\n",
      "359 10.9 %\n",
      "360 9.6 %\n",
      "361 10.4 %\n",
      "362 9.7 %\n",
      "363 10.7 %\n",
      "364 10.9 %\n",
      "365 9.4 %\n",
      "366 9.5 %\n",
      "367 9.1 %\n",
      "368 9.5 %\n",
      "369 9.9 %\n",
      "370 9.7 %\n",
      "371 10.7 %\n",
      "372 8.8 %\n",
      "373 10.5 %\n",
      "374 8.9 %\n",
      "375 8.5 %\n",
      "376 9.8 %\n",
      "377 10.2 %\n",
      "378 8.8 %\n",
      "379 11.8 %\n",
      "380 9.8 %\n",
      "381 10.5 %\n",
      "382 9.9 %\n",
      "383 10.6 %\n",
      "384 11.0 %\n",
      "385 10.5 %\n",
      "386 10.0 %\n",
      "387 9.8 %\n",
      "388 9.5 %\n",
      "389 9.4 %\n",
      "390 9.9 %\n",
      "391 10.0 %\n",
      "392 12.2 %\n",
      "393 10.0 %\n",
      "394 9.1 %\n",
      "395 9.0 %\n",
      "396 10.8 %\n",
      "397 9.8 %\n",
      "398 10.2 %\n",
      "399 10.6 %\n",
      "400 10.1 %\n",
      "401 11.1 %\n",
      "402 11.0 %\n",
      "403 10.6 %\n",
      "404 10.4 %\n",
      "405 10.8 %\n",
      "406 9.7 %\n",
      "407 8.9 %\n",
      "408 9.5 %\n",
      "409 9.6 %\n",
      "410 8.0 %\n",
      "411 9.4 %\n",
      "412 10.0 %\n",
      "413 11.0 %\n",
      "414 9.6 %\n",
      "415 9.4 %\n",
      "416 9.0 %\n",
      "417 8.6 %\n",
      "418 10.6 %\n",
      "419 10.4 %\n",
      "420 11.3 %\n",
      "421 11.1 %\n",
      "422 9.4 %\n",
      "423 11.1 %\n",
      "424 10.0 %\n",
      "425 10.1 %\n",
      "426 11.0 %\n",
      "427 10.9 %\n",
      "428 10.6 %\n",
      "429 10.5 %\n",
      "430 10.0 %\n",
      "431 9.5 %\n",
      "432 11.0 %\n",
      "433 9.3 %\n",
      "434 10.2 %\n",
      "435 11.2 %\n",
      "436 10.6 %\n",
      "437 9.3 %\n",
      "438 8.6 %\n",
      "439 12.4 %\n",
      "440 10.6 %\n",
      "441 10.5 %\n",
      "442 9.6 %\n",
      "443 11.5 %\n",
      "444 10.3 %\n",
      "445 9.6 %\n",
      "446 9.8 %\n",
      "447 9.8 %\n",
      "448 11.4 %\n",
      "449 10.8 %\n",
      "450 11.0 %\n",
      "451 9.9 %\n",
      "452 10.8 %\n",
      "453 10.8 %\n",
      "454 9.8 %\n",
      "455 12.0 %\n",
      "456 9.9 %\n",
      "457 11.3 %\n",
      "458 10.2 %\n",
      "459 10.0 %\n",
      "460 9.5 %\n",
      "461 10.1 %\n",
      "462 9.3 %\n",
      "463 9.9 %\n",
      "464 11.7 %\n",
      "465 10.4 %\n",
      "466 9.1 %\n",
      "467 10.6 %\n",
      "468 9.6 %\n",
      "469 10.7 %\n",
      "470 10.7 %\n",
      "471 9.4 %\n",
      "472 11.1 %\n",
      "473 10.3 %\n",
      "474 10.0 %\n",
      "475 9.2 %\n",
      "476 10.8 %\n",
      "477 11.9 %\n",
      "478 10.0 %\n",
      "479 9.8 %\n",
      "480 9.2 %\n",
      "481 10.7 %\n",
      "482 9.2 %\n",
      "483 9.3 %\n",
      "484 10.2 %\n",
      "485 11.1 %\n",
      "486 8.8 %\n",
      "487 9.4 %\n",
      "488 9.6 %\n",
      "489 9.3 %\n",
      "490 8.7 %\n",
      "491 10.4 %\n",
      "492 8.5 %\n",
      "493 9.5 %\n",
      "494 9.0 %\n",
      "495 10.2 %\n",
      "496 10.2 %\n",
      "497 10.4 %\n",
      "498 10.2 %\n",
      "499 9.2 %\n",
      "500 10.8 %\n",
      "501 9.7 %\n",
      "502 10.0 %\n",
      "503 9.3 %\n",
      "504 10.0 %\n",
      "505 10.2 %\n",
      "506 11.0 %\n",
      "507 11.1 %\n",
      "508 11.2 %\n",
      "509 10.5 %\n",
      "510 9.8 %\n",
      "511 10.2 %\n",
      "512 10.5 %\n",
      "513 9.2 %\n",
      "514 9.5 %\n",
      "515 10.6 %\n",
      "516 10.1 %\n",
      "517 10.6 %\n",
      "518 9.2 %\n",
      "519 10.0 %\n",
      "520 9.4 %\n",
      "521 9.9 %\n",
      "522 11.4 %\n",
      "523 8.9 %\n",
      "524 9.1 %\n",
      "525 9.9 %\n",
      "526 9.5 %\n",
      "527 11.1 %\n",
      "528 9.4 %\n",
      "529 10.2 %\n",
      "530 10.9 %\n",
      "531 8.8 %\n",
      "532 9.3 %\n",
      "533 11.1 %\n",
      "534 8.7 %\n",
      "535 10.0 %\n",
      "536 10.3 %\n",
      "537 9.0 %\n",
      "538 9.5 %\n",
      "539 9.1 %\n",
      "540 10.7 %\n",
      "541 9.9 %\n",
      "542 11.3 %\n",
      "543 9.3 %\n",
      "544 8.4 %\n",
      "545 9.9 %\n",
      "546 8.4 %\n",
      "547 9.9 %\n",
      "548 10.8 %\n",
      "549 8.6 %\n",
      "550 10.7 %\n",
      "551 10.6 %\n",
      "552 9.1 %\n",
      "553 10.3 %\n",
      "554 10.8 %\n",
      "555 9.5 %\n",
      "556 10.7 %\n",
      "557 9.6 %\n",
      "558 11.2 %\n",
      "559 10.2 %\n",
      "560 9.6 %\n",
      "561 9.5 %\n",
      "562 9.2 %\n",
      "563 9.5 %\n",
      "564 10.5 %\n",
      "565 9.6 %\n",
      "566 9.3 %\n",
      "567 11.1 %\n",
      "568 9.8 %\n",
      "569 11.1 %\n",
      "570 10.8 %\n",
      "571 10.6 %\n",
      "572 10.3 %\n",
      "573 8.8 %\n",
      "574 9.9 %\n",
      "575 10.4 %\n",
      "576 12.0 %\n",
      "577 10.6 %\n",
      "578 9.7 %\n",
      "579 9.2 %\n",
      "580 9.1 %\n",
      "581 10.1 %\n",
      "582 9.6 %\n",
      "583 11.1 %\n",
      "584 9.0 %\n",
      "585 11.4 %\n",
      "586 10.5 %\n",
      "587 8.8 %\n",
      "588 9.9 %\n",
      "589 10.1 %\n",
      "590 11.7 %\n",
      "591 9.0 %\n",
      "592 8.7 %\n",
      "593 10.7 %\n",
      "594 10.2 %\n",
      "595 10.4 %\n",
      "596 11.0 %\n",
      "597 8.4 %\n",
      "598 9.8 %\n",
      "599 11.1 %\n",
      "600 10.0 %\n",
      "601 10.9 %\n",
      "602 10.9 %\n",
      "603 10.4 %\n",
      "604 10.3 %\n",
      "605 9.4 %\n",
      "606 10.4 %\n",
      "607 9.5 %\n",
      "608 10.0 %\n",
      "609 8.9 %\n",
      "610 9.9 %\n",
      "611 8.4 %\n",
      "612 9.2 %\n",
      "613 9.7 %\n",
      "614 10.0 %\n",
      "615 10.8 %\n",
      "616 9.3 %\n",
      "617 9.3 %\n",
      "618 11.4 %\n",
      "619 11.1 %\n",
      "620 9.6 %\n",
      "621 9.7 %\n",
      "622 9.5 %\n",
      "623 9.0 %\n",
      "624 10.4 %\n",
      "625 9.2 %\n",
      "626 10.6 %\n",
      "627 9.9 %\n",
      "628 9.7 %\n",
      "629 9.6 %\n",
      "630 9.2 %\n",
      "631 9.9 %\n",
      "632 9.1 %\n",
      "633 10.5 %\n",
      "634 10.9 %\n",
      "635 9.7 %\n",
      "636 11.6 %\n",
      "637 9.3 %\n",
      "638 9.5 %\n",
      "639 9.6 %\n",
      "640 9.5 %\n",
      "641 10.2 %\n",
      "642 9.9 %\n",
      "643 9.2 %\n",
      "644 10.0 %\n",
      "645 8.6 %\n",
      "646 11.0 %\n",
      "647 11.6 %\n",
      "648 8.7 %\n",
      "649 10.7 %\n",
      "650 11.0 %\n",
      "651 10.1 %\n",
      "652 9.9 %\n",
      "653 9.8 %\n",
      "654 10.1 %\n",
      "655 10.5 %\n",
      "656 10.3 %\n",
      "657 10.2 %\n",
      "658 9.6 %\n",
      "659 10.6 %\n",
      "660 9.8 %\n",
      "661 10.0 %\n",
      "662 9.5 %\n",
      "663 9.7 %\n",
      "664 11.3 %\n",
      "665 9.3 %\n",
      "666 8.5 %\n",
      "667 9.5 %\n",
      "668 10.0 %\n",
      "669 9.4 %\n",
      "670 11.8 %\n",
      "671 12.2 %\n",
      "672 8.1 %\n",
      "673 8.9 %\n",
      "674 9.4 %\n",
      "675 10.6 %\n",
      "676 11.1 %\n",
      "677 9.0 %\n",
      "678 10.3 %\n",
      "679 8.6 %\n",
      "680 9.0 %\n",
      "681 10.1 %\n",
      "682 9.4 %\n",
      "683 10.5 %\n",
      "684 10.4 %\n",
      "685 11.4 %\n",
      "686 10.4 %\n",
      "687 9.4 %\n",
      "688 9.7 %\n",
      "689 8.3 %\n",
      "690 10.8 %\n",
      "691 10.5 %\n",
      "692 9.0 %\n",
      "693 11.2 %\n",
      "694 10.6 %\n",
      "695 9.9 %\n",
      "696 10.8 %\n",
      "697 11.0 %\n",
      "698 10.0 %\n",
      "699 9.9 %\n",
      "700 10.1 %\n",
      "701 10.1 %\n",
      "702 11.6 %\n",
      "703 10.8 %\n",
      "704 12.0 %\n",
      "705 11.5 %\n",
      "706 10.8 %\n",
      "707 10.0 %\n",
      "708 10.0 %\n",
      "709 9.5 %\n",
      "710 10.5 %\n",
      "711 9.1 %\n",
      "712 11.6 %\n",
      "713 9.3 %\n",
      "714 9.8 %\n",
      "715 9.0 %\n",
      "716 11.0 %\n",
      "717 9.3 %\n",
      "718 11.9 %\n",
      "719 11.0 %\n",
      "720 8.4 %\n",
      "721 10.9 %\n",
      "722 9.9 %\n",
      "723 8.9 %\n",
      "724 10.2 %\n",
      "725 8.8 %\n",
      "726 9.8 %\n",
      "727 9.8 %\n",
      "728 11.0 %\n",
      "729 9.8 %\n",
      "730 9.4 %\n",
      "731 8.7 %\n",
      "732 9.3 %\n",
      "733 9.5 %\n",
      "734 9.1 %\n",
      "735 9.9 %\n",
      "736 10.7 %\n",
      "737 10.4 %\n",
      "738 10.4 %\n",
      "739 11.1 %\n",
      "740 9.1 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-f87d92349c5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mffnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mffnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mffnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-b50795e55e64>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_layer_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[1;31m# First layer weights update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[0mderror_w1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnd_out\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnd_out\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnd_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[0mderror_w1\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb_epochs = 1000\n",
    "nb_batch = 5\n",
    "nb_rows_in_batch = int(nb_epochs / nb_batch)\n",
    "\n",
    "for i in range(nb_epochs):\n",
    "    X, Y = shuffle(X_train, Y_train)\n",
    "    \n",
    "    for j in range(nb_batch):\n",
    "        X_batch = X[:(j + 1) * nb_rows_in_batch]\n",
    "        Y_batch = Y[:(j + 1) * nb_rows_in_batch]\n",
    "        \n",
    "        ffnn.forward(X_batch, Y_batch)\n",
    "        \n",
    "        ffnn.backward()\n",
    "    \n",
    "    print(i, ffnn.accuracy(), '%')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
